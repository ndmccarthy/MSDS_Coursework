{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-7.4.4-py3-none-any.whl (325 kB)\n",
      "     |████████████████████████████████| 325 kB 24.1 MB/s            \n",
      "\u001b[?25hCollecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.2.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest) (20.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.6.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions>=4.6.0\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->pytest) (1.14.0)\n",
      "Installing collected packages: typing-extensions, tomli, pluggy, iniconfig, exceptiongroup, pytest\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.2\n",
      "    Uninstalling typing-extensions-3.7.4.2:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.2\n",
      "Successfully installed exceptiongroup-1.3.0 iniconfig-2.0.0 pluggy-1.2.0 pytest-7.4.4 tomli-2.0.1 typing-extensions-4.7.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use sklearn's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_stand = scaler.fit_transform(X)\n",
    "        return X_stand\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data (still shape m x n)\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        mean_vec = np.mean(X_std, axis  = 0)\n",
    "        return mean_vec\n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        # C = (1/(n-1))(X^T)(X)\n",
    "        n = X_std.shape[1]\n",
    "        trans_x = np.transpose(X_std)\n",
    "        XtX = np.matmul(trans_x, X_std)\n",
    "        return (1 / (n - 1)) * XtX\n",
    "                    \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat: covariance matrix (n x n)\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        (eigen_values, eigen_vector) = np.linalg.eig(cov_mat)\n",
    "        return (eigen_values, eigen_vector)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals: list of eigen values \n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return eigen_vals / sum(eigen_vals)\n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumulative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        last_id = np.searchsorted(cum_var_exp, self.target_explained_variance)\n",
    "        weight_mat = np.column_stack([eig_pairs[ii][1] for ii in range(last_id)])\n",
    "        return weight_mat\n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit function returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # your code here\n",
    "        X_stand = self.standardize(X)\n",
    "        mean_vec = self.compute_mean_vector(X_stand)\n",
    "        cov_mat = self.compute_cov(X_stand, mean_vec)\n",
    "        eig_vals, eig_vecs = self.compute_eigen_vector(cov_mat)\n",
    "        explain_var = self.compute_explained_variance(eig_vals)\n",
    "        cumul_sum = self.cumulative_sum(explain_var)\n",
    "        eig_tups = list(zip(eig_vals, eig_vecs))\n",
    "        matrix_w = self.compute_weight_matrix(eig_tups, cumul_sum)\n",
    "        \n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_stand, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[ PART A ]** Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.76210598, -0.16204713, -0.30782581, ..., -0.59559713,\n",
       "         1.89589423, -1.82267976],\n",
       "       [-0.67412994, -1.4731584 ,  0.98797076, ..., -0.54229945,\n",
       "         1.10653478, -0.07527283],\n",
       "       [-0.37635124,  0.4795273 ,  0.05450683, ..., -0.52267721,\n",
       "         0.43099326, -0.45225727],\n",
       "       ...,\n",
       "       [ 0.73048138, -0.33194437,  0.1496125 , ..., -0.79454014,\n",
       "         0.5541192 , -1.13042876],\n",
       "       [ 1.2396341 ,  0.97842286, -0.02260214, ..., -0.23556245,\n",
       "        -0.21095542, -0.93819761],\n",
       "       [-1.54195825, -0.41098849, -1.13214613, ...,  0.21914664,\n",
       "        -0.25598711,  0.92150652]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)\n",
    "X_train_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7f483253e7756e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sample Testing of implemented functions\n",
    "\n",
    "Use the below cells one by one to test each of your functions implemented above. <br>\n",
    "This should serve handy for debugging sections of your code <br>\n",
    "\n",
    "**Please Note - The hidden tests on which the actual points are awarded are different from these and usually run on a different, more complex dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69de0b2e5db3b495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.array([[0.39, 1.07, 0.06, 0.79], [-1.15, -0.51, -0.21, -0.7], [-1.36, 0.57, 0.37, 0.09], [0.06, 1.04, 0.99, -1.78]])\n",
    "pca_handler = PCA(target_explained_variance=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c373a031a46afda",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_std_act = pca_handler.standardize(X)\n",
    "\n",
    "X_std_exp = [[ 1.20216033, 0.82525828, -0.54269609, 1.24564656],\n",
    "             [-0.84350476, -1.64660539, -1.14693504, -0.31402854],\n",
    "             [-1.1224591, 0.04302294, 0.15105974, 0.51291329],\n",
    "             [ 0.76380353, 0.77832416, 1.53857139, -1.4445313]]\n",
    "\n",
    "for act, exp in zip(X_std_act, X_std_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check Standardize function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07e377f58487a992",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_vec_act = pca_handler.compute_mean_vector(X_std_act)\n",
    "\n",
    "mean_vec_exp = [5.55111512, 2.77555756, 5.55111512, -5.55111512]\n",
    "\n",
    "mean_vec_act_tmp = mean_vec_act * 1e17\n",
    "\n",
    "assert pytest.approx(mean_vec_act_tmp, 0.1) == mean_vec_exp, \"Check compute_mean_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-58c635786e1213c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cov_mat_act = pca_handler.compute_cov(X_std_act, mean_vec_act) \n",
    "\n",
    "cov_mat_exp = [[ 1.33333333, 0.97573583, 0.44021511, 0.02776305],\n",
    " [ 0.97573583, 1.33333333, 0.88156376, 0.14760488],\n",
    " [ 0.44021511, 0.88156376, 1.33333333, -0.82029039],\n",
    " [ 0.02776305, 0.14760488, -0.82029039, 1.33333333]]\n",
    "\n",
    "assert pytest.approx(cov_mat_act, 0.01) == cov_mat_exp, \"Check compute_cov function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-62ec2c97eac4b335",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eig_vals_act, eig_vecs_act = pca_handler.compute_eigen_vector(cov_mat_act) \n",
    "\n",
    "eig_vals_exp = [2.96080083e+00, 1.80561744e+00, 5.66915059e-01, 7.86907276e-17]\n",
    "\n",
    "eig_vecs_exp = [[ 0.50989282,  0.38162981,  0.72815056,  0.25330765],\n",
    " [ 0.59707545,  0.33170546, -0.37363029, -0.62759286],\n",
    " [ 0.57599397, -0.37480162, -0.41446394,  0.59663585],\n",
    " [-0.22746684,  0.77708038, -0.3980161,   0.43126337]]\n",
    "\n",
    "assert pytest.approx(eig_vals_act, 0.01) == eig_vals_exp, \"Check compute_eigen_vector function\"\n",
    "\n",
    "for act, exp in zip(eig_vecs_act, eig_vecs_exp):\n",
    "    assert pytest.approx(act, 0.01) == exp, \"Check compute_eigen_vector function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-14c3cd051410c833",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca_handler.feature_size = X.shape[1]\n",
    "var_exp_act = pca_handler.compute_explained_variance(eig_vals_act) \n",
    "\n",
    "var_exp_exp = [0.5551501556710813, 0.33855327084133857, 0.10629657348758019, 1.475451142706682e-17]\n",
    "\n",
    "assert pytest.approx(var_exp_act, 0.01) == var_exp_exp, \"Check compute_explained_variance function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b5a71e28a40a6cc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eig_pairs = np.array([(2.9608008302457662, np.array([ 0.50989282,  0.59707545,  0.57599397, -0.22746684])),\n",
    "(1.8056174444871387, np.array([ 0.38162981,  0.33170546, -0.37480162,  0.77708038])),\n",
    "(0.5669150586004276, np.array([ 0.72815056, -0.37363029, -0.41446394, -0.3980161 ])), \n",
    "(7.869072761102302e-17, np.array([ 0.25330765, -0.62759286,  0.59663585,  0.43126337]))])\n",
    "\n",
    "cum_var_exp = [0.55515016, 0.89370343, 1, 1]\n",
    "\n",
    "matrix_w_exp = [[0.50989282, 0.38162981], \n",
    "                [0.59707545, 0.33170546], \n",
    "                [0.57599397, -0.37480162], \n",
    "                [-0.22746684, 0.77708038]]\n",
    "\n",
    "matrix_w_act = pca_handler.compute_weight_matrix(eig_pairs=eig_pairs, cum_var_exp=cum_var_exp)\n",
    "\n",
    "for act, exp in zip(matrix_w_act, matrix_w_exp):\n",
    "    assert pytest.approx(act, 0.001) == exp, \"Check compute_weight_matrix function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abb4fbdac246eac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Result Comparison with Sklearn\n",
    "\n",
    "The below cells should help you compare the output from your implementation against the sklearn implementation with a similar configuration. This is solely to help you validate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5ff5b5144a624a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84828343,  0.41253192],\n",
       "       [-1.97317794, -0.42421224],\n",
       "       [-0.3159959 , -1.03426304],\n",
       "       [ 1.44089042,  1.04594337]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this cell to verify against the sklearn implementation given in the next cell\n",
    "mean_vec = pca_handler.compute_mean_vector(X_std_act)\n",
    "cov_mat = pca_handler.compute_cov(X_std_act, mean_vec)\n",
    "eig_vals, eig_vecs = pca_handler.compute_eigen_vector(cov_mat)\n",
    "explain_var = pca_handler.compute_explained_variance(eig_vals)\n",
    "cumul_sum = pca_handler.cumulative_sum(explain_var)\n",
    "eig_tups = list(zip(eig_vals, eig_vecs))\n",
    "matrix_w = pca_handler.compute_weight_matrix(eig_tups, cumul_sum)\n",
    "pca_handler.transform_data(X_std=X_std_act, matrix_w=matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef8d927f60236e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: [[ 0.39  1.07  0.06  0.79]\n",
      " [-1.15 -0.51 -0.21 -0.7 ]\n",
      " [-1.36  0.57  0.37  0.09]\n",
      " [ 0.06  1.04  0.99 -1.78]]\n",
      "\n",
      "Scaled data: [[ 1.20216033  0.82525828 -0.54269609  1.24564656]\n",
      " [-0.84350476 -1.64660539 -1.14693504 -0.31402854]\n",
      " [-1.1224591   0.04302294  0.15105974  0.51291329]\n",
      " [ 0.76380353  0.77832416  1.53857139 -1.4445313 ]]\n",
      "\n",
      "Transformed Data [[ 0.50978142  1.90389378]\n",
      " [-2.00244127 -0.68224688]\n",
      " [-0.57630715 -0.07213548]\n",
      " [ 2.068967   -1.14951141]]\n"
     ]
    }
   ],
   "source": [
    "# Sklearn implementation to compare your results\n",
    "\n",
    "# import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA as pca1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data before applying PCA\n",
    "scaling=StandardScaler()\n",
    " \n",
    "# Use fit and transform method\n",
    "# You may change the variable X if needed to verify against a different dataset\n",
    "print(\"Sample data:\", X)\n",
    "scaling.fit(X)\n",
    "Scaled_data=scaling.transform(X)\n",
    "print(\"\\nScaled data:\", Scaled_data)\n",
    " \n",
    "# Set the n_components=3\n",
    "principal=pca1(n_components=2)\n",
    "principal.fit(Scaled_data)\n",
    "x=principal.transform(Scaled_data)\n",
    " \n",
    "# Check the dimensions of data after PCA\n",
    "print(\"\\nTransformed Data\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
